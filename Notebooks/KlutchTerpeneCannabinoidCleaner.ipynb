{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'UsersKlutch_lorain_products_10_25_2023 - Lorain_10_23_2023 (2).csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the first CSV file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m csv_file_path1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsersKlutch_lorain_products_10_25_2023 - Lorain_10_23_2023 (2).csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the second CSV file\u001b[39;00m\n\u001b[1;32m     10\u001b[0m csv_file_path2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsers/johnaffolter/downloads/Klutch_canton_products_10_23_2023 - canton_products_10_23_2023.csv (2).csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'UsersKlutch_lorain_products_10_25_2023 - Lorain_10_23_2023 (2).csv'"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the first CSV file\n",
    "csv_file_path1 = 'UsersKlutch_lorain_products_10_25_2023 - Lorain_10_23_2023 (2).csv'\n",
    "df1 = pd.read_csv(csv_file_path1)\n",
    "\n",
    "# Load the second CSV file\n",
    "csv_file_path2 = 'Users/johnaffolter/downloads/Klutch_canton_products_10_23_2023 - canton_products_10_23_2023.csv (2).csv'\n",
    "df2 = pd.read_csv(csv_file_path2)\n",
    "\n",
    "# Show a snippet of each dataframe to understand the data structure\n",
    "df1.head(), df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'retail_store_id' and convert each group to a JSON file\n",
    "def save_groups_to_json(df, file_prefix):\n",
    "    grouped = df.groupby('retail_store_id')\n",
    "    json_file_paths = []\n",
    "    \n",
    "    for name, group in grouped:\n",
    "        output_json = {\n",
    "            \"retail_store_id\": int(name),\n",
    "            \"input\": group.drop(columns=['retail_store_id']).to_dict(orient='records')\n",
    "        }\n",
    "        \n",
    "        # Save the JSON to a file\n",
    "        json_file_path = f\"/mnt/data/{file_prefix}_store_{int(name)}.json\"\n",
    "        with open(json_file_path, 'w') as f:\n",
    "            json.dump(output_json, f, indent=4)\n",
    "        \n",
    "        json_file_paths.append(json_file_path)\n",
    "        \n",
    "    return json_file_paths\n",
    "\n",
    "# Convert and save the first dataframe to JSON files\n",
    "json_file_paths1 = save_groups_to_json(df1, 'Lorain')\n",
    "\n",
    "# Convert and save the second dataframe to JSON files\n",
    "json_file_paths2 = save_groups_to_json(df2, 'Canton')\n",
    "\n",
    "json_file_paths1, json_file_paths2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN and None values with an empty string in the dataframes\n",
    "df1.fillna(\"\", inplace=True)\n",
    "df2.fillna(\"\", inplace=True)\n",
    "\n",
    "# Re-run the conversion process to update the JSON files\n",
    "json_file_paths1 = save_groups_to_json(df1, 'Lorain')\n",
    "json_file_paths2 = save_groups_to_json(df2, 'Canton')\n",
    "\n",
    "json_file_paths1, json_file_paths2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the nested lists in 'labResults' column into separate columns\n",
    "def flatten_lab_results(row):\n",
    "    lab_results = row['labResults']\n",
    "    flattened = {}\n",
    "    if lab_results:\n",
    "        for result in lab_results:\n",
    "            test = result['labTest']\n",
    "            value = result['value']\n",
    "            unit = result['labResultUnit']\n",
    "            flattened[f\"{test}_value\"] = value\n",
    "            flattened[f\"{test}_unit\"] = unit\n",
    "    return pd.Series(flattened)\n",
    "\n",
    "# Apply the function to flatten 'labResults' and concatenate it to the original DataFrame\n",
    "flattened_lab_results = df.apply(flatten_lab_results, axis=1)\n",
    "df_flattened = pd.concat([df, flattened_lab_results], axis=1)\n",
    "\n",
    "# Drop the original 'labResults' column\n",
    "df_flattened.drop(columns=['labResults'], inplace=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame with flattened lab results\n",
    "df_flattened.head()\n",
    "\n",
    "\n",
    "# For columns that have units in other columns, we create new columns to specify the unit of measurement\n",
    "df_flattened['unitWeight_with_unit'] = df_flattened['unitWeight'].astype(str) + \" \" + df_flattened['unitWeightUnit']\n",
    "df_flattened['unitPrice_with_unit'] = df_flattened['unitPrice'].astype(str) + \" USD\"\n",
    "df_flattened['medUnitPrice_with_unit'] = df_flattened['medUnitPrice'].astype(str) + \" USD\"\n",
    "df_flattened['recUnitPrice_with\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Section 1 loads the JSON data into a Python object.\n",
    "\n",
    "##Section 2 converts this Python object to a Pandas DataFrame.\n",
    "\n",
    "##Section 3 flattens the nested labResults column.\n",
    "\n",
    "##Section 4 parses out numerical details into their own columns with the correct units of measurement.\n",
    "\n",
    "##Section 5 saves the processed DataFrame to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Section 1: Load JSON Data\n",
    "# -------------------------\n",
    "with open('path/to/your/GetInventoryKlutchRaw.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Section 2: Convert JSON to DataFrame\n",
    "# ------------------------------------\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Section 3: Flatten Nested Columns (labResults)\n",
    "# ----------------------------------------------\n",
    "def flatten_lab_results(row):\n",
    "    lab_results = row['labResults']\n",
    "    flattened = {}\n",
    "    if lab_results:\n",
    "        for result in lab_results:\n",
    "            test = result['labTest']\n",
    "            value = result['value']\n",
    "            unit = result['labResultUnit']\n",
    "            flattened[f\"{test}_value\"] = value\n",
    "            flattened[f\"{test}_unit\"] = unit\n",
    "    return pd.Series(flattened)\n",
    "\n",
    "flattened_lab_results = df.apply(flatten_lab_results, axis=1)\n",
    "df_flattened = pd.concat([df, flattened_lab_results], axis=1)\n",
    "df_flattened.drop(columns=['labResults'], inplace=True)\n",
    "\n",
    "# Section 4: Parse Numerical Columns\n",
    "# ----------------------------------\n",
    "df_flattened['unitWeight_with_unit'] = df_flattened['unitWeight'].astype(str) + \" \" + df_flattened['unitWeightUnit']\n",
    "df_flattened['unitPrice_with_unit'] = df_flattened['unitPrice'].astype(str) + \" USD\"\n",
    "df_flattened['medUnitPrice_with_unit'] = df_flattened['medUnitPrice'].astype(str) + \" USD\"\n",
    "df_flattened['recUnitPrice_with_unit'] = df_flattened['recUnitPrice'].astype(str) + \" USD\"\n",
    "df_flattened['effectivePotencyMg_with_unit'] = df_flattened['effectivePotencyMg'].astype(str) + \" mg\"\n",
    "\n",
    "# Section 5: Save to CSV\n",
    "# -----------------------\n",
    "df_flattened.to_csv('Processed_Inventory_Data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Section 1: Load JSON Data\n",
    "# -------------------------\n",
    "with open('Users/johnaffolter/Downloads/GetInventoryKlutchRaw.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Section 2: Convert JSON to DataFrame\n",
    "# ------------------------------------\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Section 3: Flatten Nested Columns (labResults)\n",
    "# ----------------------------------------------\n",
    "def flatten_lab_results(row):\n",
    "    lab_results = row['labResults']\n",
    "    flattened = {}\n",
    "    if lab_results:\n",
    "        for result in lab_results:\n",
    "            test = result['labTest']\n",
    "            value = result['value']\n",
    "            unit = result['labResultUnit']\n",
    "            flattened[f\"{test}_value\"] = value\n",
    "            flattened[f\"{test}_unit\"] = unit\n",
    "    return pd.Series(flattened)\n",
    "\n",
    "flattened_lab_results = df.apply(flatten_lab_results, axis=1)\n",
    "df_flattened = pd.concat([df, flattened_lab_results], axis=1)\n",
    "df_flattened.drop(columns=['labResults'], inplace=True)\n",
    "\n",
    "# Section 4: Parse Numerical Columns\n",
    "# ----------------------------------\n",
    "df_flattened['unitWeight_with_unit'] = df_flattened['unitWeight'].astype(str) + \" \" + df_flattened['unitWeightUnit']\n",
    "df_flattened['unitPrice_with_unit'] = df_flattened['unitPrice'].astype(str) + \" USD\"\n",
    "df_flattened['medUnitPrice_with_unit'] = df_flattened['medUnitPrice'].astype(str) + \" USD\"\n",
    "df_flattened['recUnitPrice_with_unit'] = df_flattened['recUnitPrice'].astype(str) + \" USD\"\n",
    "df_flattened['effectivePotencyMg_with_unit'] = df_flattened['effectivePotencyMg'].astype(str) + \" mg\"\n",
    "\n",
    "# Section 5: Extract Numerical Units from Other Columns\n",
    "# -----------------------------------------------------\n",
    "def extract_numerical_and_units(s):\n",
    "    if pd.isna(s):\n",
    "        return None, None\n",
    "    parts = s.split(' ')\n",
    "    if len(parts) == 2:\n",
    "        return parts[0], parts[1]\n",
    "    return None, None\n",
    "\n",
    "# Assuming the column 'size' contains entries like \"2.83g\" or \"110.0 g\"\n",
    "# Uncomment this line if your data has a 'size' column.\n",
    "# df_flattened['size_numerical'], df_flattened['size_unit'] = zip(*df_flattened['size'].map(extract_numerical_and_units))\n",
    "\n",
    "# Section 6: Save to CSV\n",
    "# -----------------------\n",
    "df_flattened.to_csv('users/johnaffolter/Downloads/Processed_Inventory_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unitWeightUnit</th>\n",
       "      <th>productId</th>\n",
       "      <th>sku</th>\n",
       "      <th>productName</th>\n",
       "      <th>description</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>category</th>\n",
       "      <th>imageUrl</th>\n",
       "      <th>quantityAvailable</th>\n",
       "      <th>quantityUnits</th>\n",
       "      <th>...</th>\n",
       "      <th>Terpinolene_value</th>\n",
       "      <th>TransNerolidol_unit</th>\n",
       "      <th>TransNerolidol_value</th>\n",
       "      <th>pCymene_unit</th>\n",
       "      <th>pCymene_value</th>\n",
       "      <th>unitWeight_with_unit</th>\n",
       "      <th>unitPrice_with_unit</th>\n",
       "      <th>medUnitPrice_with_unit</th>\n",
       "      <th>recUnitPrice_with_unit</th>\n",
       "      <th>effectivePotencyMg_with_unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g</td>\n",
       "      <td>714462</td>\n",
       "      <td>M00000348804</td>\n",
       "      <td>Ice Cream Cake</td>\n",
       "      <td>(Wedding Cake x Gelato #33) \\n\\nBred by @SeedJ...</td>\n",
       "      <td>17333</td>\n",
       "      <td>Flower</td>\n",
       "      <td>https://leaflogixmedia.blob.core.windows.net/p...</td>\n",
       "      <td>31</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Percentage</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Percentage</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.83 g</td>\n",
       "      <td>36.0 USD</td>\n",
       "      <td>36.0 USD</td>\n",
       "      <td>36.0 USD</td>\n",
       "      <td>848.4 mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g</td>\n",
       "      <td>723270</td>\n",
       "      <td>M00000446909</td>\n",
       "      <td>Camino 3:1 Sparkling Pear Gummies</td>\n",
       "      <td>6mg CBD / 2mg THC per gummy.\\n\\nKick back and ...</td>\n",
       "      <td>18219</td>\n",
       "      <td>Gummies</td>\n",
       "      <td>https://leaflogixmedia.blob.core.windows.net/p...</td>\n",
       "      <td>6</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0 g</td>\n",
       "      <td>22.0 USD</td>\n",
       "      <td>22.0 USD</td>\n",
       "      <td>22.0 USD</td>\n",
       "      <td>40.0 mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g</td>\n",
       "      <td>943972</td>\n",
       "      <td>M00000364861</td>\n",
       "      <td>Sherbhead</td>\n",
       "      <td>(Loompas Headband x Sherbet)\\n\\nBred by Cannar...</td>\n",
       "      <td>17333</td>\n",
       "      <td>Flower</td>\n",
       "      <td>https://leaflogixmedia.blob.core.windows.net/p...</td>\n",
       "      <td>36</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Percentage</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Percentage</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.83 g</td>\n",
       "      <td>36.0 USD</td>\n",
       "      <td>36.0 USD</td>\n",
       "      <td>36.0 USD</td>\n",
       "      <td>798.9 mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g</td>\n",
       "      <td>1056309</td>\n",
       "      <td>72178660</td>\n",
       "      <td>Pax Mini - Charcoal</td>\n",
       "      <td>PAX Mini is the smallest PAX device yet and he...</td>\n",
       "      <td>18339</td>\n",
       "      <td>Devices</td>\n",
       "      <td>https://leaflogixmedia.blob.core.windows.net/p...</td>\n",
       "      <td>4</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0 g</td>\n",
       "      <td>149.0 USD</td>\n",
       "      <td>149.0 USD</td>\n",
       "      <td>149.0 USD</td>\n",
       "      <td>nan mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g</td>\n",
       "      <td>1092017</td>\n",
       "      <td>M00000496508</td>\n",
       "      <td>Petra Pineapple Mints</td>\n",
       "      <td>2.5mg per mint\\n\\nExercise your willpower with...</td>\n",
       "      <td>18222</td>\n",
       "      <td>Hard Candy</td>\n",
       "      <td>https://leaflogixmedia.blob.core.windows.net/p...</td>\n",
       "      <td>7</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0 g</td>\n",
       "      <td>17.0 USD</td>\n",
       "      <td>17.0 USD</td>\n",
       "      <td>17.0 USD</td>\n",
       "      <td>100.0 mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>g</td>\n",
       "      <td>1092865</td>\n",
       "      <td>M00000474604</td>\n",
       "      <td>Petra Tart Cherry Mints</td>\n",
       "      <td>2.5mg per mint\\n\\nThis robust, delectable flav...</td>\n",
       "      <td>18222</td>\n",
       "      <td>Hard Candy</td>\n",
       "      <td>https://leaflogixmedia.blob.core.windows.net/p...</td>\n",
       "      <td>14</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0 g</td>\n",
       "      <td>17.0 USD</td>\n",
       "      <td>17.0 USD</td>\n",
       "      <td>17.0 USD</td>\n",
       "      <td>100.0 mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>g</td>\n",
       "      <td>1093933</td>\n",
       "      <td>M00000501310</td>\n",
       "      <td>Camino High Dose Wild Cherry Gummies</td>\n",
       "      <td>27.5mg THC per gummy\\n\\nGet the rooftop party ...</td>\n",
       "      <td>18219</td>\n",
       "      <td>Gummies</td>\n",
       "      <td>https://leaflogixmedia.blob.core.windows.net/p...</td>\n",
       "      <td>11</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>550.0 g</td>\n",
       "      <td>70.0 USD</td>\n",
       "      <td>70.0 USD</td>\n",
       "      <td>70.0 USD</td>\n",
       "      <td>550.0 mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>g</td>\n",
       "      <td>1094140</td>\n",
       "      <td>M00000355412</td>\n",
       "      <td>Motorbreath</td>\n",
       "      <td>(SFV OG x Chem D)\\n\\nWhile the breeder of this...</td>\n",
       "      <td>17333</td>\n",
       "      <td>Flower</td>\n",
       "      <td>https://leaflogixmedia.blob.core.windows.net/p...</td>\n",
       "      <td>34</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Percentage</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Percentage</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.83 g</td>\n",
       "      <td>36.0 USD</td>\n",
       "      <td>36.0 USD</td>\n",
       "      <td>36.0 USD</td>\n",
       "      <td>867.1 mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>g</td>\n",
       "      <td>1095052</td>\n",
       "      <td>25599602</td>\n",
       "      <td>Puffco Plus</td>\n",
       "      <td>The most awarded and flavorful concentrate vap...</td>\n",
       "      <td>18339</td>\n",
       "      <td>Devices</td>\n",
       "      <td>https://leaflogixmedia.blob.core.windows.net/p...</td>\n",
       "      <td>6</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0 g</td>\n",
       "      <td>89.0 USD</td>\n",
       "      <td>89.0 USD</td>\n",
       "      <td>89.0 USD</td>\n",
       "      <td>nan mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>g</td>\n",
       "      <td>1112861</td>\n",
       "      <td>M00000364863</td>\n",
       "      <td>Sherbhead</td>\n",
       "      <td>(Loompas Headband x Sherbet) \\n\\nBred by Canna...</td>\n",
       "      <td>17333</td>\n",
       "      <td>Flower</td>\n",
       "      <td>https://leaflogixmedia.blob.core.windows.net/p...</td>\n",
       "      <td>15</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Percentage</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Percentage</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.15 g</td>\n",
       "      <td>140.0 USD</td>\n",
       "      <td>140.0 USD</td>\n",
       "      <td>140.0 USD</td>\n",
       "      <td>4362.4 mg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  unitWeightUnit  productId           sku  \\\n",
       "0              g     714462  M00000348804   \n",
       "1              g     723270  M00000446909   \n",
       "2              g     943972  M00000364861   \n",
       "3              g    1056309      72178660   \n",
       "4              g    1092017  M00000496508   \n",
       "5              g    1092865  M00000474604   \n",
       "6              g    1093933  M00000501310   \n",
       "7              g    1094140  M00000355412   \n",
       "8              g    1095052      25599602   \n",
       "9              g    1112861  M00000364863   \n",
       "\n",
       "                            productName  \\\n",
       "0                        Ice Cream Cake   \n",
       "1     Camino 3:1 Sparkling Pear Gummies   \n",
       "2                             Sherbhead   \n",
       "3                   Pax Mini - Charcoal   \n",
       "4                 Petra Pineapple Mints   \n",
       "5               Petra Tart Cherry Mints   \n",
       "6  Camino High Dose Wild Cherry Gummies   \n",
       "7                           Motorbreath   \n",
       "8                           Puffco Plus   \n",
       "9                             Sherbhead   \n",
       "\n",
       "                                         description  categoryId    category  \\\n",
       "0  (Wedding Cake x Gelato #33) \\n\\nBred by @SeedJ...       17333      Flower   \n",
       "1  6mg CBD / 2mg THC per gummy.\\n\\nKick back and ...       18219     Gummies   \n",
       "2  (Loompas Headband x Sherbet)\\n\\nBred by Cannar...       17333      Flower   \n",
       "3  PAX Mini is the smallest PAX device yet and he...       18339     Devices   \n",
       "4  2.5mg per mint\\n\\nExercise your willpower with...       18222  Hard Candy   \n",
       "5  2.5mg per mint\\n\\nThis robust, delectable flav...       18222  Hard Candy   \n",
       "6  27.5mg THC per gummy\\n\\nGet the rooftop party ...       18219     Gummies   \n",
       "7  (SFV OG x Chem D)\\n\\nWhile the breeder of this...       17333      Flower   \n",
       "8  The most awarded and flavorful concentrate vap...       18339     Devices   \n",
       "9  (Loompas Headband x Sherbet) \\n\\nBred by Canna...       17333      Flower   \n",
       "\n",
       "                                            imageUrl  quantityAvailable  \\\n",
       "0  https://leaflogixmedia.blob.core.windows.net/p...                 31   \n",
       "1  https://leaflogixmedia.blob.core.windows.net/p...                  6   \n",
       "2  https://leaflogixmedia.blob.core.windows.net/p...                 36   \n",
       "3  https://leaflogixmedia.blob.core.windows.net/p...                  4   \n",
       "4  https://leaflogixmedia.blob.core.windows.net/p...                  7   \n",
       "5  https://leaflogixmedia.blob.core.windows.net/p...                 14   \n",
       "6  https://leaflogixmedia.blob.core.windows.net/p...                 11   \n",
       "7  https://leaflogixmedia.blob.core.windows.net/p...                 34   \n",
       "8  https://leaflogixmedia.blob.core.windows.net/p...                  6   \n",
       "9  https://leaflogixmedia.blob.core.windows.net/p...                 15   \n",
       "\n",
       "  quantityUnits  ...  Terpinolene_value  TransNerolidol_unit  \\\n",
       "0      Quantity  ...               0.01           Percentage   \n",
       "1      Quantity  ...                NaN                  NaN   \n",
       "2      Quantity  ...               0.02           Percentage   \n",
       "3      Quantity  ...                NaN                  NaN   \n",
       "4      Quantity  ...                NaN                  NaN   \n",
       "5      Quantity  ...                NaN                  NaN   \n",
       "6      Quantity  ...                NaN                  NaN   \n",
       "7      Quantity  ...               0.02           Percentage   \n",
       "8      Quantity  ...                NaN                  NaN   \n",
       "9      Quantity  ...               0.01           Percentage   \n",
       "\n",
       "  TransNerolidol_value pCymene_unit  pCymene_value unitWeight_with_unit  \\\n",
       "0                 0.02   Percentage            0.0               2.83 g   \n",
       "1                  NaN          NaN            NaN              110.0 g   \n",
       "2                 0.02   Percentage            0.0               2.83 g   \n",
       "3                  NaN          NaN            NaN                0.0 g   \n",
       "4                  NaN          NaN            NaN              110.0 g   \n",
       "5                  NaN          NaN            NaN              110.0 g   \n",
       "6                  NaN          NaN            NaN              550.0 g   \n",
       "7                 0.03   Percentage            0.0               2.83 g   \n",
       "8                  NaN          NaN            NaN                0.0 g   \n",
       "9                 0.02   Percentage            0.0              14.15 g   \n",
       "\n",
       "  unitPrice_with_unit medUnitPrice_with_unit  recUnitPrice_with_unit  \\\n",
       "0            36.0 USD               36.0 USD                36.0 USD   \n",
       "1            22.0 USD               22.0 USD                22.0 USD   \n",
       "2            36.0 USD               36.0 USD                36.0 USD   \n",
       "3           149.0 USD              149.0 USD               149.0 USD   \n",
       "4            17.0 USD               17.0 USD                17.0 USD   \n",
       "5            17.0 USD               17.0 USD                17.0 USD   \n",
       "6            70.0 USD               70.0 USD                70.0 USD   \n",
       "7            36.0 USD               36.0 USD                36.0 USD   \n",
       "8            89.0 USD               89.0 USD                89.0 USD   \n",
       "9           140.0 USD              140.0 USD               140.0 USD   \n",
       "\n",
       "   effectivePotencyMg_with_unit  \n",
       "0                      848.4 mg  \n",
       "1                       40.0 mg  \n",
       "2                      798.9 mg  \n",
       "3                        nan mg  \n",
       "4                      100.0 mg  \n",
       "5                      100.0 mg  \n",
       "6                      550.0 mg  \n",
       "7                      867.1 mg  \n",
       "8                        nan mg  \n",
       "9                     4362.4 mg  \n",
       "\n",
       "[10 rows x 118 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flattened.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_flattened' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cannabinoids_terpenes_str\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create new columns for grouped cannabinoids and terpenes\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m df_flattened[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannabinoids_Terpenes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_flattened\u001b[49m\u001b[38;5;241m.\u001b[39mapply(group_cannabinoids_terpenes, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Display the first few rows to verify that the new column has been added correctly\u001b[39;00m\n\u001b[1;32m     19\u001b[0m df_flattened[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannabinoids_Terpenes\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_flattened' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to group all cannabinoids and terpenes along with their units into a string\n",
    "def group_cannabinoids_terpenes(row):\n",
    "    cannabinoids_terpenes_str = ''\n",
    "    for col in df_flattened.columns:\n",
    "        if '_value' in col:\n",
    "            value_col = col\n",
    "            unit_col = col.replace('_value', '_unit')\n",
    "            test_name = col.replace('_value', '')\n",
    "            value = row[value_col]\n",
    "            unit = row[unit_col]\n",
    "            if pd.notna(value) and pd.notna(unit):\n",
    "                cannabinoids_terpenes_str += f\"{test_name}: {value} {unit}, \"\n",
    "    return cannabinoids_terpenes_str.rstrip(', ')\n",
    "\n",
    "# Create new columns for grouped cannabinoids and terpenes\n",
    "df_flattened['Cannabinoids_Terpenes'] = df_flattened.apply(group_cannabinoids_terpenes, axis=1)\n",
    "\n",
    "# Display the first few rows to verify that the new column has been added correctly\n",
    "df_flattened[['Cannabinoids_Terpenes']].head()\n",
    "df_flattened.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEAN CODE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Users/johnaffolter/Downloads/GetInventoryKlutchRaw.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load JSON Data\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUsers/johnaffolter/Downloads/GetInventoryKlutchRaw.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Convert list to DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/opt/ipython/libexec/lib/python3.11/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Users/johnaffolter/Downloads/GetInventoryKlutchRaw.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load JSON Data\n",
    "with open('Users/johnaffolter/Downloads/GetInventoryKlutchRaw.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to flatten the 'labResults' column\n",
    "def flatten_lab_results(row):\n",
    "    lab_results = row.get('labResults', [])\n",
    "    flattened = {}\n",
    "    if lab_results:\n",
    "        for result in lab_results:\n",
    "            test = result['labTest']\n",
    "            value = result['value']\n",
    "            unit = result['labResultUnit']\n",
    "            flattened[f\"{test}_value\"] = value\n",
    "            flattened[f\"{test}_unit\"] = unit\n",
    "    return pd.Series(flattened)\n",
    "\n",
    "# Apply the flattening function to the DataFrame\n",
    "flattened_lab_results = df.apply(flatten_lab_results, axis=1)\n",
    "df_flattened = pd.concat([df, flattened_lab_results], axis=1)\n",
    "df_flattened.drop(columns=['labResults'], inplace=True)\n",
    "\n",
    "# Extended list of known cannabinoids and terpenes for more comprehensive matching\n",
    "extended_known_cannabinoids = ['THC', 'THCA', 'CBD', 'CBDA', 'CBN', 'CBG', 'CBGA', 'CBC', 'THCV', 'CBDV']\n",
    "extended_known_terpenes = ['Myrcene', 'Limonene', 'Caryophyllene', 'AlphaPinene', 'BetaPinene', 'Humulene', 'Linalool', 'Terpinolene', 'Ocimene', 'Nerolidol']\n",
    "\n",
    "# Update the function to use the extended lists of known cannabinoids and terpenes\n",
    "def group_specific_compounds_updated(row, known_compounds):\n",
    "    compound_str = ''\n",
    "    for col in df_flattened.columns:\n",
    "        if '_value' in col:\n",
    "            test_name = col.replace('_value', '')\n",
    "            if test_name in known_compounds:\n",
    "                value_col = col\n",
    "                unit_col = col.replace('_value', '_unit')\n",
    "                value = row[value_col]\n",
    "                unit = row[unit_col]\n",
    "                if pd.notna(value) and pd.notna(unit):\n",
    "                    unit = \"%\" if unit.lower() == \"percentage\" else unit\n",
    "                    compound_str += f\"{test_name}: {value} {unit}, \"\n",
    "    return compound_str.rstrip(', ')\n",
    "\n",
    "# Update the columns for grouped cannabinoids and terpenes using the extended lists\n",
    "df_flattened['Grouped_Cannabinoids'] = df_flattened.apply(lambda row: group_specific_compounds_updated(row, extended_known_cannabinoids), axis=1)\n",
    "df_flattened['Grouped_Terpenes'] = df_flattened.apply(lambda row: group_specific_compounds_updated(row, extended_known_terpenes), axis=1)\n",
    "\n",
    "# Save the final DataFrame to CSV\n",
    "df_flattened.to_csv('Users/johnaffolter/Downloads/Final_Processed_Inventory_Data.csv', index=False)\n",
    "df_flattened.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
